import boto3
import pandas as pd
from io import StringIO # python3: python2: BytesIO

#defining all parametrs

AWS_ACCESS_KEY ='AKIAQM5PKG2LN2JKRLHR'
AWS_SECRET_KEY ='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'
AWS_REGION ='ap-south-1'
SCHEMA_NAME ='covid_dataset' # athena database name
S3_STAGING_DIR ='s3://vishnu-test-buck/output/' # query output will stored in output folder
S3_BUCKET_NAME ='vishnu-test-buck'
S3_OUTPUT_DIRECTORY ='output'


# connecting to athena for query

athena_client =boto3.client('athena',
	aws_access_key_id=AWS_ACCESS_KEY,
	aws_secret_access_key=AWS_SECRET_KEY,
	region_name=AWS_RGION,)




#defining function

Dict={}

def download_and_load_query_results(
    client:boto3.client,query_response:Dict
) ->pd.DataFrame:
    while True:
        try:
            # This function only loads the first 1000 rows
            client.get_query_results(
            QueryExecutionId=query_response['QueryExecutionId']

            )
            break
        except Exceptionn as err:
            if 'not yet finished' in str(err):
                time.sleep(0.001)
            else:
                raise err
    temp_file_location:str ='athena_query_results.csv'
    s3_client =boto3.client(
        's3',
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=AWS_REGION,
    )

    s3_client.download_file(
        S3_BUCKET_NAME,
        f"{S3_OUTPUT_DIRECTORY}/{query_response['QueryExecutionId']}.csv",
        temp_file_loaction,
    )

    return pd.read_csv(temp_file_location)







#querying athena (table1 -enigma_jhud)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM enigma_jhud",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)



response

df_data=download_and_load_query_results(athena_client,response)

engima_jhud.head()



#querying athena (table2 -nytimes_data_in_usa_countryus_county)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM nytimes_data_in_usa_countryus_county",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)


nytimes_data_in_usa_countryus_county=download_and_load_query_results(athena_client,response)


nytimes_data_in_usa_countryus_county.head()




#querying athena (table3 -nytimes_data_in_usa_us_states)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM nytimes_data_in_usa_us_states",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)


nytimes_data_in_usa_us_states=download_and_load_query_results(athena_client,response)


nytimes_data_in_usa_us_states.head()


#querying athena (table4 -rearc_covid_19_testing_data_states_daily)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM rearc_covid_19_testing_data_states_daily",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)

rearc_covid_19_testing_data_states_daily=download_and_load_query_results(athena_client,response)

rearc_covid_19_testing_data_states_daily.head()


#querying athena (table5 -rearc_covid_19_testing_data_us_daily)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM rearc_covid_19_testing_data_us_daily",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)


rearc_covid_19_testing_data_us_daily=download_and_load_query_results(athena_client,response)

rearc_covid_19_testing_data_us_daily.head()



#querying athena (table6 -rearc_covid_19_testing_data_us_total_latest)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM rearc_covid_19_testing_data_us_total_latest",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)


rearc_covid_19_testing_data_us_total_latest=download_and_load_query_results(athena_client,response)

rearc_covid_19_testing_data_us_total_latest.head()


#querying athena (table8 -static_datasets_countrycode)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM static_datasets_countrycode",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)


static_datasets_countrycode=download_and_load_query_results(athena_client,response)

static_datasets_countrycode.head()




#querying athena (table9 -static_datasets_countypopulation)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM static_datasets_countypopulation",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)


static_datasets_countypopulation=download_and_load_query_results(athena_client,response)

static_datasets_countypopulation.head()

#querying athena (table10 -static_datasets_state_abv)

response =athena_client.start_query_execution(
    QueryString="SELECT * FROM static_datasets_state_abv",
    QueryExecutionContext={"Database":SCHEMA_NAME},
    ResultConfiguration={
        "OutputLocation":S3_STAGING_DIR,
        "EncryptionConfiguration":{"EncryptionOption":"SSE_S3"},
    },
)


static_datasets_state_abv=download_and_load_query_results(athena_client,response)

static_datasets_state_abv.head()

----------------------------------------------------------------------------------
# removing first row from table1 beause its showing c0 and c1

new_header=static_datasets_state_abv.iloc[0] #grab the first row the header

new_header


static_datasets_state_abv=static_datasets_state_abv[1:]# take the data less the header row

static_datasets_state_abv.columns=new_header# set the header row as the df header

static_datasets_state_abv.head()

-----------------------------------------------------------------------------------------




#joining two tables base on requremet check diemseison image we requred factCovid
#here we used pandas



factCovid_1 =engima_jhud[['fips','province_state',	'country_region','confirmed','deaths','recovered','active']]

factCovid2=rearc_covid_19_testing_data_states_daily[['fips','date','positive','negative','hospitalizedcurrently','hospitalized','hospitalizeddischarged']]

factCovid=pd.merge(factCovid_1,factCovid2,on='fips',how='inner')

factCovid.head()

factCovid.shape



#dimregiontable (accroding to diemension image)

dimRegion_1=enigma_jhud[['fips','province_state','country_region','latitude','longitude','']]
dimRegion_2=nytimes_data_in_usa_countryus_county[['fips','county','state']]
dimRegion=pd.merge(dimRegion_1,dimRegion_2,on'fips',how='inner')


dimRegion.head()

dimRegion.shape

--------------------------------------
#dimhopitaltable (according to diemesnion)
dimhospital=rearc_usa_hospital_beds[['fips','state_name','latitude','longtitude','hq_address','hospital_name','hospital_type','hq_city','hq_state']]

dimhospital.head()

dimhospital.shape
----------------------------------------------------

#dimDate table (according to diemesnion)
dimDate=rearc_covid_19_testing_data_states_daily[['fips',date]]

dimDate.head()

dimDate.shape
-------------------------------------------
#converting date from dimDate


dimDate['date'] = pd.to_datetime(dimDate['date'],format="%Y%m%d")



#to define ,year,month,day_of_week
dimDate['year']=dimDate['date'].dt.year
dimDate['month']=dimDate['date'].dt.month
dimDate['day_of_week']=dimDate['date'].dt.dayofweek

------------------------------------------------------------
#error in this code
#now saving all diemansion model table into s3

bucket='vishnu-covid-de-project' #already created on s3

csv_buffer =StringIO()
csv_buffer
factCovid.to_csv(csv_buffer)


s3_resource=boto3.resource('s3')
s3_resource.Object(bucket,'output/factCovid.csv').put(Body=csv_buffer.getvalue())


------------------------------------------------------------------------------------------------



#solution
#table ==>factCovid
#Creating Session With Boto3.

session = boto3.Session(
aws_access_key_id=AWS_ACCESS_KEY,
aws_secret_access_key=AWS_SECRET_KEY
)
#Creating S3 Resource From the Session.
s3_res = session.resource('s3')
csv_buffer = StringIO()
factCovid.to_csv(csv_buffer)
bucket_name = 'vishnu-covid-de-project'
s3_object_name = 'factCovid.csv'
s3_res.Object(bucket_name, 'output/factCovid.csv').put(Body=csv_buffer.getvalue())
print(" factCovid is saved as CSV in S3 bucket.")

-------------------------------------------------


#solution
#table ==>dimRegion (it would take one hour to write beacuse size is 2.4gb)

#Creating S3 Resource From the Session.
s3_res = session.resource('s3')
csv_buffer = StringIO()
dimRegion.to_csv(csv_buffer)
bucket_name = 'vishnu-covid-de-project'
s3_object_name = 'output/dimRegion.csv'
s3_res.Object(bucket_name, s3_object_name).put(Body=csv_buffer.getvalue())
print(" dimRegion is saved as CSV in S3 bucket.")

-------------------------------------------------------------------------



#solution
#table ==>dimhospital

#Creating S3 Resource From the Session.
s3_res = session.resource('s3')
csv_buffer = StringIO()
dimhospital.to_csv(csv_buffer)
bucket_name = 'vishnu-covid-de-project'
s3_object_name = 'output/dimhospital.csv'
s3_res.Object(bucket_name, s3_object_name).put(Body=csv_buffer.getvalue())
print(" dimhospital is saved as CSV in S3 bucket.")


----------------------------------------------------------------------------------------


#solution
#table ==>dimDate


#Creating S3 Resource From the Session.
s3_res = session.resource('s3')
csv_buffer = StringIO()
dimDate.to_csv(csv_buffer)
bucket_name = 'vishnu-covid-de-project'
s3_object_name = 'output/dimDate.csv'
s3_res.Object(bucket_name, s3_object_name).put(Body=csv_buffer.getvalue())
print(" dimDate is saved as CSV in S3 bucket.")

---------------------------------------------------------------------------------------

# creating a schema of above four tables in pandas

dimDatesql=pd.io.sql.get_schema(dimDate.reset_index(),'dimDate')
print(''.join(dimDatesql))


#table===>dimhospital
dimhospitalsql=pd.io.sql.get_schema(dimhospital.reset_index(),'dimhospital')
print(''.join(dimhospitalsql))


#table===>factCovid 
factCovidsql=pd.io.sql.get_schema(factCovid.reset_index(),'factCovid')
print(''.join(factCovidsql))



#skipping this one beacuse it take long time and also stuck my computer
#table===>dimRegion 
dimRegionsql=pd.io.sql.get_schema(dimRegion.reset_index(),'dimRegion')
print(''.join(dimRegionsql))


------------------------------------------------------------------------


import redshift_connector

#now creating a connection betwween python and redshift


conn = redshift_connector.connect(
    host='redshift-cluster-1.cafwxanap8ta.ap-south-1.redshift.amazonaws.com',
    database='dev',
    user="awsuser",
    password='Sahil1234'
)

conn.autocommit=True


cursor=redshift_connector.Cursor = conn.cursor()

--------------------------------------------------------

#now creating table in redshift
#table-->dimDate

cursor.execute("""
CREATE TABLE "dimDate"(
"index" INTEGER,
  "fips" REAL,
  "date" TIMESTAMP,
  "year" INTEGER,
  "month" INTEGER,
  "day_of_week" INTEGER
)
""")




#table-->dimhospital

cursor.execute("""
CREATE TABLE "dimhospital" (
"index" INTEGER,
  "fips" REAL,
  "state_name" TEXT,
  "latitude" REAL,
  "longtitude" REAL,
  "hq_address" TEXT,
  "hospital_name" TEXT,
  "hospital_type" TEXT,
  "hq_city" TEXT,
  "hq_state" TEXT
)
""")




#table-->factCovid

cursor.execute("""
CREATE TABLE "factCovid" (
"index" INTEGER,
  "fips" REAL,
  "province_state" TEXT,
  "country_region" TEXT,
  "confirmed" REAL,
  "deaths" REAL,
  "recovered" REAL,
  "active" REAL,
  "date" INTEGER,
  "positive" INTEGER,
  "negative" REAL,
  "hospitalizedcurrently" REAL,
  "hospitalized" REAL,
  "hospitalizeddischarged" REAL
)
""")





#table-->dimRegion

cursor.execute("""
CREATE TABLE "dimRegion" (
"index" INTEGER,
  "fips" REAL,
  "province_state" TEXT,
  "country_region" TEXT,
  "latitude" REAL,
  "longitude" REAL,
  "county" TEXT,
  "state" TEXT
  
)
""")
-------------------------------------------------------------------------------------------------------------------------